{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T06:48:11.607600Z",
     "start_time": "2018-07-20T06:48:11.601022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Shared': {'CodeDimension': '500'}, 'Optimizer': {'ReportTrainLossEvery': '100', 'MaxGradientNorm': '1', 'Algorithm': {'learning_rate': '0.01', 'Name': 'Adam'}, 'EarlyStopping': {'CheckEvery': '100', 'BurninPhaseDuration': '6000'}}, 'Evaluation': {'Metric': 'MRR'}, 'Encoder': {'AddDiagonal': 'No', 'RandomInput': 'No', 'NumberOfBasisFunctions': '5', 'UseOutputTransform': 'No', 'SkipConnections': 'None', 'Name': 'gcn_basis', 'Concatenation': 'No', 'DiagonalCoefficients': 'No', 'NumberOfLayers': '2', 'DropoutKeepProbability': '0.8', 'StoreEdgeData': 'No', 'UseInputTransform': 'Yes', 'InternalEncoderDimension': '500', 'PartiallyRandomInput': 'No'}, 'General': {'ExperimentName': 'models/GcnBlock', 'GraphSplitSize': '0.5', 'NegativeSampleRate': '10'}, 'Decoder': {'RegularizationParameter': '0.01', 'Name': 'bilinear-diag'}}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "sys.path.append('./code')\n",
    "sys.path.append('./code/optimization')\n",
    "\n",
    "from optimization.optimize import build_tensorflow\n",
    "from common import settings_reader, io, model_builder, optimizer_parameter_parser, evaluation, auxilliaries\n",
    "import numpy as np\n",
    "\n",
    "setting_file = './settings/gcn_basis.exp'\n",
    "dataset = './data/Toy/'\n",
    "settings = settings_reader.read(setting_file)\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T06:51:37.500690Z",
     "start_time": "2018-07-20T06:51:37.493009Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "1. Load datasets\n",
    "'''\n",
    "relations_path = dataset + '/relations.dict'\n",
    "entities_path = dataset + '/entities.dict'\n",
    "train_path = dataset + '/train.txt'\n",
    "valid_path = dataset + '/valid.txt'\n",
    "test_path = dataset + '/test.txt'\n",
    "\n",
    "#Extend paths for accuracy evaluation:\n",
    "if settings['Evaluation']['Metric'] == 'Accuracy':\n",
    "    valid_path = dataset + '/valid_accuracy.txt'\n",
    "    test_path = dataset + '/test_accuracy.txt'\n",
    "\n",
    "train_triplets = io.read_triplets_as_list(train_path, entities_path, relations_path)\n",
    "valid_triplets = io.read_triplets_as_list(valid_path, entities_path, relations_path)\n",
    "test_triplets = io.read_triplets_as_list(test_path, entities_path, relations_path)\n",
    "\n",
    "train_triplets = np.array(train_triplets)\n",
    "valid_triplets = np.array(valid_triplets)\n",
    "test_triplets = np.array(test_triplets)\n",
    "\n",
    "entities = io.read_dictionary(entities_path)\n",
    "relations = io.read_dictionary(relations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T04:51:51.514602Z",
     "start_time": "2018-07-20T04:51:51.506524Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "2. Load general settings\n",
    "'''\n",
    "encoder_settings = settings['Encoder']\n",
    "decoder_settings = settings['Decoder']\n",
    "shared_settings = settings['Shared']\n",
    "general_settings = settings['General']\n",
    "optimizer_settings = settings['Optimizer']\n",
    "evaluation_settings = settings['Evaluation']\n",
    "\n",
    "general_settings.put('EntityCount', len(entities))\n",
    "general_settings.put('RelationCount', len(relations))\n",
    "general_settings.put('EdgeCount', len(train_triplets))\n",
    "\n",
    "encoder_settings.merge(shared_settings)\n",
    "encoder_settings.merge(general_settings)\n",
    "decoder_settings.merge(shared_settings)\n",
    "decoder_settings.merge(general_settings)\n",
    "\n",
    "optimizer_settings.merge(general_settings)\n",
    "evaluation_settings.merge(general_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T07:20:50.104809Z",
     "start_time": "2018-07-20T07:20:50.098535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<class 'encoders.relation_embedding.RelationEmbedding'> <class 'decoders.bilinear_diag.BilinearDiag'>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "3. Construct the encoder-decoder pair:\n",
    "'''\n",
    "# common.model_builder.build_encoder() -> extras.graph_representations.Representation -> encoders.affine_transform.AffineTransform ->\n",
    "#      common.model_builder.apply_basis_gcn() -> encoders.relation_embedding.RelationEmbedding\n",
    "encoder = model_builder.build_encoder(encoder_settings, train_triplets) # RelationEmbedding object inheriting from Model\n",
    "\n",
    "# decoders.bilinear_diag.BilinearDiag object inheriting from Model\n",
    "model = model_builder.build_decoder(encoder, decoder_settings) # next_component = encoder\n",
    "\n",
    "print(encoder.needs_graph()) \n",
    "print(type(encoder), type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T07:34:46.832516Z",
     "start_time": "2018-07-20T07:34:46.814008Z"
    },
    "code_folding": [
     29,
     52,
     80,
     153
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "train -> step 4 -> scorer(evaluation.Scorer)\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "4. Construct the optimizer with validation MRR as early stopping metric:\n",
    "'''\n",
    "opp = optimizer_parameter_parser.Parser(optimizer_settings)\n",
    "opp.set_save_function(model.save) # BilinearDiag -> Model.save\n",
    "\n",
    "scorer = evaluation.Scorer(evaluation_settings)\n",
    "scorer.register_data(train_triplets)\n",
    "scorer.register_data(valid_triplets)\n",
    "scorer.register_data(test_triplets)\n",
    "scorer.register_degrees(train_triplets)\n",
    "scorer.register_model(model)\n",
    "scorer.finalize_frequency_computation(np.concatenate((train_triplets, valid_triplets, test_triplets), axis=0))\n",
    "print('---------------------------------------------------------------')\n",
    "print('  train -> step 4 -> scorer(evaluation.Scorer)')\n",
    "#print('  known_subject_triples', scorer.known_subject_triples)\n",
    "#print('  known_object_triples', scorer.known_object_triples)\n",
    "#print('  relation_freqs', scorer.relation_freqs)\n",
    "#print('  avg_freq', scorer.avg_freq)\n",
    "print('---------------------------------------------------------------')\n",
    "\n",
    "def score_validation_data(validation_data):\n",
    "    score_summary = scorer.compute_scores(validation_data, verbose=False).get_summary()\n",
    "    #score_summary.dump_degrees('dumps/degrees.in', 'dumps/degrees.out')\n",
    "    #score_summary.dump_frequencies('dumps/near.freq', 'dumps/target.freq')\n",
    "    #score_summary.pretty_print()\n",
    "\n",
    "    if evaluation_settings['Metric'] == 'MRR':\n",
    "        lookup_string = score_summary.mrr_string()\n",
    "    elif evaluation_settings['Metric'] == 'Accuracy':\n",
    "        lookup_string = score_summary.accuracy_string()\n",
    "\n",
    "    early_stopping = score_summary.results['Filtered'][lookup_string]\n",
    "\n",
    "    score_summary = scorer.compute_scores(test_triplets, verbose=False).get_summary()\n",
    "    score_summary.pretty_print()\n",
    "\n",
    "    return early_stopping\n",
    "\n",
    "opp.set_early_stopping_score_function(score_validation_data)\n",
    "\n",
    "adj_list = [[] for _ in entities]\n",
    "for i,triplet in enumerate(train_triplets):   # undirected graph ?\n",
    "    adj_list[triplet[0]].append([i, triplet[2]])\n",
    "    adj_list[triplet[2]].append([i, triplet[0]])\n",
    "degrees = np.array([len(a) for a in adj_list])\n",
    "adj_list = [np.array(a) for a in adj_list]\n",
    "#print(train_triplets)\n",
    "#print(adj_list)\n",
    "#print(degrees)\n",
    "\n",
    "# not in use\n",
    "def sample_TIES(triplets, n_target_vertices):\n",
    "    vertex_set = set([])\n",
    "\n",
    "    edge_indices = np.arange(triplets.shape[0])\n",
    "    while len(vertex_set) < n_target_vertices:\n",
    "        edge = triplets[np.random.choice(edge_indices)]\n",
    "        new_vertices = [edge[0], edge[1]]\n",
    "        vertex_set = vertex_set.union(new_vertices)\n",
    "\n",
    "    sampled = [False]*triplets.shape[0]\n",
    "\n",
    "    for i in edge_indices:\n",
    "        edge = triplets[i]\n",
    "        if edge[0] in vertex_set and edge[2] in vertex_set:\n",
    "            sampled[i] = True\n",
    "\n",
    "    return edge_indices[sampled]\n",
    "\n",
    "\n",
    "def sample_edge_neighborhood(triplets, sample_size):\n",
    "\n",
    "    edges = np.zeros((sample_size), dtype=np.int32)\n",
    "\n",
    "    #initialize\n",
    "    sample_counts = np.array([d for d in degrees])\n",
    "    picked = np.array([False for _ in triplets])\n",
    "    seen = np.array([False for _ in degrees])\n",
    "\n",
    "    for i in range(0, sample_size):\n",
    "        weights = sample_counts * seen\n",
    "\n",
    "        if np.sum(weights) == 0:\n",
    "            weights = np.ones_like(weights)\n",
    "            weights[np.where(sample_counts == 0)] = 0\n",
    "\n",
    "        probabilities = (weights) / np.sum(weights)\n",
    "        chosen_vertex = np.random.choice(np.arange(degrees.shape[0]), p=probabilities)\n",
    "        chosen_adj_list = adj_list[chosen_vertex]\n",
    "        seen[chosen_vertex] = True\n",
    "\n",
    "        chosen_edge = np.random.choice(np.arange(chosen_adj_list.shape[0]))\n",
    "        chosen_edge = chosen_adj_list[chosen_edge]\n",
    "        edge_number = chosen_edge[0]\n",
    "\n",
    "        while picked[edge_number]:\n",
    "            chosen_edge = np.random.choice(np.arange(chosen_adj_list.shape[0]))\n",
    "            chosen_edge = chosen_adj_list[chosen_edge]\n",
    "            edge_number = chosen_edge[0]\n",
    "\n",
    "        edges[i] = edge_number\n",
    "        other_vertex = chosen_edge[1]\n",
    "        picked[edge_number] = True\n",
    "        sample_counts[chosen_vertex] -= 1\n",
    "        sample_counts[other_vertex] -= 1\n",
    "        seen[other_vertex] = True\n",
    "\n",
    "    return edges\n",
    "\n",
    "\n",
    "if 'NegativeSampleRate' in general_settings:\n",
    "    ns = auxilliaries.NegativeSampler(int(general_settings['NegativeSampleRate']), general_settings['EntityCount'])\n",
    "    ns.set_known_positives(train_triplets)\n",
    "\n",
    "    def t_func(x): #horrible hack!!!\n",
    "        arr = np.array(x)\n",
    "        #RelationEmbedding <- model.needs_graph() <- RelationEmbedding.next_component.needs_graph() <- \n",
    "        #    BasisGcn <- MessageGcn.needs_graph: True\n",
    "        if not encoder.needs_graph():\n",
    "            return ns.transform(arr)\n",
    "        else:\n",
    "            if 'GraphBatchSize' in general_settings:\n",
    "                graph_batch_size = int(general_settings['GraphBatchSize'])\n",
    "\n",
    "                '''\n",
    "                n = np.zeros(100)\n",
    "                for i in range(100):\n",
    "                    if i % 20 == 0:\n",
    "                        print(i)\n",
    "                    n[i] = sample_TIES(arr, 1000).shape[0]\n",
    "\n",
    "                print(n.mean())\n",
    "                print(n.std())\n",
    "                exit()\n",
    "                '''\n",
    "                \n",
    "                #graph_batch_ids = sample_TIES(arr, 1000) #sample_edge_neighborhood(arr, graph_batch_size)\n",
    "                graph_batch_ids = sample_edge_neighborhood(arr, graph_batch_size)\n",
    "            else:\n",
    "                graph_batch_size = arr.shape[0]\n",
    "                graph_batch_ids = np.arange(graph_batch_size)\n",
    "\n",
    "            graph_batch = np.array(train_triplets)[graph_batch_ids]\n",
    "\n",
    "            # Apply dropouts:\n",
    "            graph_percentage = float(general_settings['GraphSplitSize'])\n",
    "            split_size = int(graph_percentage * graph_batch.shape[0])\n",
    "            graph_split_ids = np.random.choice(graph_batch_ids, size=split_size, replace=False)\n",
    "            graph_split = np.array(train_triplets)[graph_split_ids]\n",
    "\n",
    "            t = ns.transform(graph_batch)\n",
    "\n",
    "            if 'StoreEdgeData' in encoder_settings and encoder_settings['StoreEdgeData'] == \"Yes\":\n",
    "                return (graph_split, graph_split_ids, t[0], t[1])\n",
    "            else:\n",
    "                return (graph_split, t[0], t[1])\n",
    "\n",
    "    opp.set_sample_transform_function(t_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T09:08:31.295255Z",
     "start_time": "2018-07-20T09:08:30.839202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "(16, 500)\n",
      "(500,)\n",
      "(500, 5, 500)\n",
      "(500, 5, 500)\n",
      "(9, 5)\n",
      "(9, 5)\n",
      "(500, 500)\n",
      "(500,)\n",
      "(500, 5, 500)\n",
      "(500, 5, 500)\n",
      "(9, 5)\n",
      "(9, 5)\n",
      "(500, 500)\n",
      "(500,)\n",
      "(16, 500)\n",
      "[<tf.Tensor 'graph_edges_1:0' shape=(?, 3) dtype=int32>, <tf.Tensor 'Placeholder_3:0' shape=(?, 3) dtype=int32>, <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "5. Initialize for training:\n",
    "'''\n",
    "\n",
    "# Hack for validation evaluation:\n",
    "model.preprocess(train_triplets)  # decoders.bilinear_diag.BilinearDiag <- Model.preprocess\n",
    "model.register_for_test(train_triplets) # call: Model.register_for_test\n",
    "\n",
    "model.initialize_train()  # decoders/bilinear_diag.initialize_train\n",
    "\n",
    "optimizer_weights = model.get_weights()\n",
    "for weight in optimizer_weights:\n",
    "    print(weight.get_shape)\n",
    "'''\n",
    "weight order\n",
    "  AffineTransform.local_get_weights: [self.W, self.b]\n",
    "  BasisGcn.local_get_weights: [self.W_forward, self.W_backward, self.C_forward, self.C_backward, self.W_self, self.b]\n",
    "  BasisGcn.local_get_weights: [self.W_forward, self.W_backward, self.C_forward, self.C_backward, self.W_self, self.b]\n",
    "  RelationEmbedding.local_get_weights: [self.W_relation]\n",
    "'''    \n",
    "    \n",
    "optimizer_input = model.get_train_input_variables() #After model.initialize_train() \n",
    "for weight in optimizer_input:\n",
    "    print(weight.get_shape)\n",
    "'''\n",
    "optimizer_input order\n",
    "  extras/graph_representations.Representation.local_get_train_input_variables: [self.X]\n",
    "  decoders/bilinear_diag.local_get_train_input_variables.local_get_train_input_variables: [self.X, self.Y]\n",
    "'''\n",
    "\n",
    "# model.get_loss()\n",
    "# decoders/bilinear_diag.BilinearDiag.get_loss() -> BilinearDiag.compute_codes() -> RelationEmbedding.get_all_codes() ->\n",
    "#      BasisGcn (MessageGcn.get_all_codes()) (两层) -> AffineTransform.get_all_codes()\n",
    "loss = model.get_loss(mode='train') + model.get_regularization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "6. Clean this shit up:\n",
    "'''\n",
    "\n",
    "for add_op in model.get_additional_ops():\n",
    "    opp.additional_ops.append(add_op)\n",
    "\n",
    "optimizer_parameters = opp.get_parametrization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build tensorflow...\n",
      "SampleTransformer\n",
      "GradientClipping\n",
      "Adam\n",
      "TrainLossReporter\n",
      "EarlyStopper\n",
      "ModelSaver\n",
      "fit...\n",
      "WARNING:tensorflow:From /home/yaoxin/.local/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:107: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initial loss: 24.8577\n",
      "Average train loss for iteration 1-100: 34.178371408\n",
      "Average train loss for iteration 101-200: 0.27878484413\n",
      "Average train loss for iteration 201-300: 0.228052216172\n",
      "Average train loss for iteration 301-400: 0.197706042677\n",
      "Average train loss for iteration 401-500: 0.188237770945\n",
      "Average train loss for iteration 501-600: 0.185940514207\n",
      "Average train loss for iteration 601-700: 0.175802872181\n",
      "Average train loss for iteration 701-800: 0.17519323796\n",
      "Average train loss for iteration 801-900: 0.175423926711\n",
      "Average train loss for iteration 901-1000: 0.16630381912\n",
      "Average train loss for iteration 1001-1100: 0.163117232621\n",
      "Average train loss for iteration 1101-1200: 0.160824085176\n",
      "Average train loss for iteration 1201-1300: 0.162100131214\n",
      "Average train loss for iteration 1301-1400: 0.161845785081\n",
      "Average train loss for iteration 1401-1500: 0.158213150352\n",
      "Average train loss for iteration 1501-1600: 0.160472963899\n",
      "Average train loss for iteration 1601-1700: 0.15894603543\n",
      "Average train loss for iteration 1701-1800: 0.159291871786\n",
      "Average train loss for iteration 1801-1900: 0.158034697324\n",
      "\tRaw\tFiltered\n",
      "MRR\t0.358\t0.43\n",
      "H@1\t0.2\t0.2\n",
      "H@3\t0.3\t0.6\n",
      "H@10\t0.8\t0.8\n",
      "Tested validation score at iteration 2000. Result: 0.429583333333\n",
      "saving...\n",
      "INFO:tensorflow:Restoring parameters from models/GcnBlock-0\n",
      "Average train loss for iteration 1901-2000: 0.158798074722\n",
      "Average train loss for iteration 2001-2100: 0.1568930085\n",
      "Average train loss for iteration 2101-2200: 0.152840154767\n",
      "Average train loss for iteration 2201-2300: 0.155177884102\n",
      "Average train loss for iteration 2301-2400: 0.153975764662\n",
      "Average train loss for iteration 2401-2500: 0.15481300965\n",
      "Average train loss for iteration 2501-2600: 0.151455136389\n",
      "Average train loss for iteration 2601-2700: 0.154420790374\n",
      "Average train loss for iteration 2701-2800: 0.151077571809\n",
      "Average train loss for iteration 2801-2900: 0.152556527406\n",
      "Average train loss for iteration 2901-3000: 0.149940696359\n",
      "Average train loss for iteration 3001-3100: 0.153136265129\n",
      "Average train loss for iteration 3101-3200: 0.153083239794\n",
      "Average train loss for iteration 3201-3300: 0.154936729595\n",
      "Average train loss for iteration 3301-3400: 0.151376342177\n",
      "Average train loss for iteration 3401-3500: 0.152601331547\n",
      "Average train loss for iteration 3501-3600: 0.157411176115\n",
      "Average train loss for iteration 3601-3700: 0.152447734326\n",
      "Average train loss for iteration 3701-3800: 0.153071438968\n",
      "Average train loss for iteration 3801-3900: 0.149339276701\n",
      "\tRaw\tFiltered\n",
      "MRR\t0.422\t0.528\n",
      "H@1\t0.3\t0.4\n",
      "H@3\t0.4\t0.6\n",
      "H@10\t0.7\t0.8\n",
      "Tested validation score at iteration 4000. Result: 0.528253968254\n",
      "saving...\n",
      "INFO:tensorflow:Restoring parameters from models/GcnBlock-1\n",
      "Average train loss for iteration 3901-4000: 0.151398882121\n",
      "Average train loss for iteration 4001-4100: 0.152181495279\n",
      "Average train loss for iteration 4101-4200: 0.149864642695\n",
      "Average train loss for iteration 4201-4300: 0.151188421994\n",
      "Average train loss for iteration 4301-4400: 0.152077797055\n",
      "Average train loss for iteration 4401-4500: 0.148354108408\n",
      "Average train loss for iteration 4501-4600: 0.151811085343\n",
      "Average train loss for iteration 4601-4700: 0.148239405006\n",
      "Average train loss for iteration 4701-4800: 0.152440803796\n",
      "Average train loss for iteration 4801-4900: 0.151677498072\n",
      "Average train loss for iteration 4901-5000: 0.150743223801\n",
      "Average train loss for iteration 5001-5100: 0.146006566286\n",
      "Average train loss for iteration 5101-5200: 0.148492344841\n",
      "Average train loss for iteration 5201-5300: 0.150107915998\n",
      "Average train loss for iteration 5301-5400: 0.150752465874\n",
      "Average train loss for iteration 5401-5500: 0.148685469255\n",
      "Average train loss for iteration 5501-5600: 0.151185410321\n",
      "Average train loss for iteration 5601-5700: 0.153936121091\n",
      "Average train loss for iteration 5701-5800: 0.150165423453\n",
      "Average train loss for iteration 5801-5900: 0.15276310727\n",
      "\tRaw\tFiltered\n",
      "MRR\t0.422\t0.528\n",
      "H@1\t0.3\t0.4\n",
      "H@3\t0.4\t0.6\n",
      "H@10\t0.7\t0.8\n",
      "Tested validation score at iteration 6000. Result: 0.528253968254\n",
      "Ignoring criterion while in burn-in phase.\n",
      "saving...\n",
      "INFO:tensorflow:Restoring parameters from models/GcnBlock-2\n",
      "Average train loss for iteration 5901-6000: 0.148660675287\n",
      "Average train loss for iteration 6001-6100: 0.14844844453\n",
      "Average train loss for iteration 6101-6200: 0.147323168293\n",
      "Average train loss for iteration 6201-6300: 0.1494188492\n",
      "Average train loss for iteration 6301-6400: 0.148207376376\n",
      "Average train loss for iteration 6401-6500: 0.148527550176\n",
      "Average train loss for iteration 6501-6600: 0.1482887806\n",
      "Average train loss for iteration 6601-6700: 0.148737489432\n",
      "Average train loss for iteration 6701-6800: 0.149677284136\n",
      "Average train loss for iteration 6801-6900: 0.150070565715\n",
      "Average train loss for iteration 6901-7000: 0.148761522248\n",
      "Average train loss for iteration 7001-7100: 0.147068122774\n",
      "Average train loss for iteration 7101-7200: 0.149125178456\n",
      "Average train loss for iteration 7201-7300: 0.148865914494\n",
      "Average train loss for iteration 7301-7400: 0.146526725739\n",
      "Average train loss for iteration 7401-7500: 0.148469874263\n",
      "Average train loss for iteration 7501-7600: 0.148395040184\n",
      "Average train loss for iteration 7601-7700: 0.14803588897\n",
      "Average train loss for iteration 7701-7800: 0.151108059585\n",
      "Average train loss for iteration 7801-7900: 0.14703650564\n",
      "\tRaw\tFiltered\n",
      "MRR\t0.363\t0.465\n",
      "H@1\t0.2\t0.3\n",
      "H@3\t0.4\t0.6\n",
      "H@10\t0.6\t0.6\n",
      "Tested validation score at iteration 8000. Result: 0.465324675325\n",
      "Stopping criterion reached.\n",
      "Stopping training.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "7. Train with Converge:\n",
    "'''\n",
    "\n",
    "model.session = tf.Session()\n",
    "print('build tensorflow...')\n",
    "optimizer = build_tensorflow(loss, optimizer_weights, optimizer_parameters, optimizer_input)\n",
    "optimizer.set_session(model.session)\n",
    "print('fit...')\n",
    "optimizer.fit(train_triplets, validation_data=valid_triplets)\n",
    "#scorer.dump_all_scores(valid_triplets, 'dumps/subjects.valid', 'dumps/objects.valid')\n",
    "#scorer.dump_all_scores(test_triplets, 'dumps/subjects.test', 'dumps/objects.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rp",
   "language": "python",
   "name": "rp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
